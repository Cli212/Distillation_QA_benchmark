# Distillation_QA_benchmark



## Description 

We aim to benchmark various distillation algorithms for QA and NER tasks. 

For QA task, you can download Natural Questions dataset from this https://ai.google.com/research/NaturalQuestions/download

Other datasets can be found here https://github.com/dmlc/gluon-nlp/tree/master/scripts/datasets/question_answering

We can put code for experiments in directory /models



## TO DOs

Benchmark TextBrewer, DynaBERT, TinyBERT in NER, classification tasks and QA task.